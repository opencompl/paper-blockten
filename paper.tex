\def\paperversiondraft{draft}
\def\paperversionnormal{normal}

% If the paper version is set to 'normal' mode keep it,
% otherwise set it to 'draft' mode.
\ifx\paperversion\paperversionnormal
\else
  \def\paperversion{draft}
\fi

\documentclass[review, anonymous, sigplan]{acmart}

\def\acmversionanonymous{anonymous}
\def\acmversionjournal{journal}
\def\acmversionnone{none}

\makeatletter
\if@ACM@anonymous
  \def\acmversion{anonymous}
\else
  \def\acmversion{journal}
\fi
\makeatother

\usepackage{colortbl}

% 'draftonly' environment
\usepackage{environ}
\ifx\paperversion\paperversiondraft
\newenvironment{draftonly}{}{}
\else
\NewEnviron{draftonly}{}
\fi

% Most PL conferences are edited by conference-publishing.com. Follow their
% advice to add the following packages.
%
% The first enables the use of UTF-8 as character encoding, which is the
% standard nowadays. The second ensures the use of font encodings that support
% accented characters etc. (Why should I use this?). The mictotype package
% enables certain features 'to­wards ty­po­graph­i­cal per­fec­tion
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{microtype}

\usepackage{xargs}
\usepackage{lipsum}
\usepackage{xparse}
\usepackage{xifthen, xstring}
\usepackage{xspace}
\usepackage{marginnote}
\usepackage{etoolbox}
\usepackage[acronym,shortcuts]{glossaries}
\usepackage{amsmath}
\usepackage{thmtools} % required for autoref to lemmas
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{hyphenat}
\usepackage[shortcuts]{extdash}

\input{tex/setup.tex}
\input{tex/acm.tex}

\usemintedstyle{colorful}

% Newer versions of minted require the 'customlexer' argument for custom lexers
% whereas older versions require the '-x' to be passed via the command line.
\makeatletter
\ifcsdef{MintedExecutable}
{
  % minted v3
  \newminted[mlir]{tools/lexers/MLIRLexer.py:MLIRLexerOnlyOps}{mathescape}
  \newminted[xdsl]{tools/lexers/MLIRLexer.py:MLIRLexer}{mathescape, style=murphy}
  \newminted[lean4]{tools/lexers/Lean4Lexer.py:Lean4Lexer}{mathescape}
}
{
  \ifcsdef{minted@optlistcl@quote}
  {
    \newminted[mlir]{tools/lexers/MLIRLexer.py:MLIRLexerOnlyOps}{customlexer, mathescape}
    \newminted[xdsl]{tools/lexers/MLIRLexer.py:MLIRLexer}{customlexer, mathescape, style=murphy}
    \newminted[lean4]{tools/lexers/Lean4Lexer.py:Lean4Lexer}{customlexer, mathescape}
  }
  {
    \newminted[mlir]{tools/lexers/MLIRLexer.py:MLIRLexerOnlyOps -x}{mathescape}
    \newminted[xdsl]{tools/lexers/MLIRLexer.py:MLIRLexer -x}{mathescape, style=murphy}
    \newminted[lean4]{tools/lexers/Lean4Lexer.py:Lean4Lexer -x}{mathescape}
  }
}
\makeatother

% We use the following color scheme
%
% This scheme is both print-friendly and colorblind safe for
% up to four colors (including the red tones makes it not
% colorblind safe any more)
%
% https://colorbrewer2.org/#type=qualitative&scheme=Paired&n=4

\definecolor{pairedNegOneLightGray}{HTML}{cacaca}
\definecolor{pairedNegTwoDarkGray}{HTML}{827b7b}
\definecolor{pairedOneLightBlue}{HTML}{a6cee3}
\definecolor{pairedTwoDarkBlue}{HTML}{1f78b4}
\definecolor{pairedThreeLightGreen}{HTML}{b2df8a}
\definecolor{pairedFourDarkGreen}{HTML}{33a02c}
\definecolor{pairedFiveLightRed}{HTML}{fb9a99}
\definecolor{pairedSixDarkRed}{HTML}{e31a1c}

\createtodoauthor{grosser}{pairedOneLightBlue}
\createtodoauthor{ivan}{pairedTwoDarkBlue}
\createtodoauthor{authorThree}{pairedThreeLightGreen}
\createtodoauthor{authorFour}{pairedFourDarkGreen}
\createtodoauthor{authorFive}{pairedFiveLightRed}
\createtodoauthor{authorSix}{pairedSixDarkRed}

\newacronym{ir}{IR}{Intermediate Representation}

\graphicspath{{./images/}}

% Define macros that are used in this paper
%
% We require all macros to end with a delimiter (by default {}) to enusure
% that LaTeX adds whitespace correctly.
\makeatletter
\newcommand\requiredelimiter[2][########]{%
  \ifdefined#2%
    \def\@temp{\def#2#1}%
    \expandafter\@temp\expandafter{#2}%
  \else
    \@latex@error{\noexpand#2undefined}\@ehc
  \fi
}
\@onlypreamble\requiredelimiter
\makeatother

\newcommand\newdelimitedcommand[2]{
\expandafter\newcommand\csname #1\endcsname{#2}
\expandafter\requiredelimiter
\csname #1 \endcsname
}

\newdelimitedcommand{toolname}{Tool}

\usepackage{booktabs}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

\usepackage[verbose]{newunicodechar}
\newunicodechar{₁}{\ensuremath{_1}}
\newunicodechar{₂}{\ensuremath{_2}}
\newunicodechar{∀}{\ensuremath{\forall}}
\newunicodechar{α}{\ensuremath{\alpha}}
\newunicodechar{β}{\ensuremath{\beta}}

% \circled command to print a colored circle.
% \circled{1} pretty-prints "(1)"
% This is useful to refer to labels that are embedded within figures.
\DeclareRobustCommand{\circled}[2][]{%
    \ifthenelse{\isempty{#1}}%
        {\circledbase{pairedOneLightBlue}{#2}}%
        {\autoref{#1}: \hyperref[#1]{\circledbase{pairedOneLightBlue}{#2}}}%
}

% listings don't write "Listing" in autoref without this.
\providecommand*{\listingautorefname}{Listing}
\renewcommand{\sectionautorefname}{Section}
\renewcommand{\subsectionautorefname}{Section}
\renewcommand{\subsubsectionautorefname}{Section}
\newcommand{\algorithmautorefname}{Algorithm}

\begin{document}

%% Title information
\title[BlockTendo]{Attention Comprehension: Extensible compiler optimization of attention kernels }       %% [Short Title] is optional;
                                      %% when present, will be used in
                                      %% header instead of Full Title.
% \subtitle{Subtitle}                   %% \subtitle is optional


%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followed by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.
\author{Ivan Ho}
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \institution{University of Cambridge}            
  \country{United Kingdom}
}
\email{ihkh2@cam.ac.uk}          %% \email is recommended

\author{Kunwar Grover}
\affiliation{
  \institution{AMD}           %% \institution is required
  \country{United Kingdom}
}
\email{TODO@amd.com}         %% \email is recommended

\author{Tobias Grosser}
\affiliation{
  \institution{University of Cambridge}            %% \institution is required
  \country{United Kingdom}
}
\email{tcg40@cam.ac.uk}          %% \email is recommended

\begin{abstract}
% An abstract should consist of six main sentences:
%  1. Introduction. In one sentence, what’s the topic?
%  2. State the problem you tackle.
%  3. Summarize (in one sentence) why nobody else has adequately answered the research question yet.
%  4. Explain, in one sentence, how you tackled the research question.
%  5. In one sentence, how did you go about doing the research that follows from your big idea.
%  6. As a single sentence, what’s the key impact of your research?

% (http://www.easterbrook.ca/steve/2010/01/how-to-write-a-scientific-abstract-in-six-easy-steps/)

  Various attention variants have risen in order to reduce the memory or computation complexity of the attention mechanism, adopting online methods, virtual paging, and many other techniques. However, kernels written at such a low-level, while efficient, are often inflexible and obscure the mathematics, making research and iteration on the attention mechanism slow and tedious. Machine learning engineers vastly prefer writing in tensor languages, which simply describe the linear algebra computations but are more difficult to optimize. We demonstrate that, by describing the tensor operations in attention as perfectly nested loops, automatic compiler optimizations and rewrites can be more easily applied, achieving the high parallelizability and low memory cost that online algorithms afford while retaining the flexibility that tensor languages afford.
\end{abstract}

% Only add ACM notes and keywords in camera ready version
% Drop citations and footnotes in draft and blind mode.
\ifx\acmversion\acmversionanonymous
\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\fi
\ifx\acmversion\acmversionjournal
%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011006.10011008</concept_id>
<concept_desc>Software and its engineering~General programming languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003456.10003457.10003521.10003525</concept_id>
<concept_desc>Social and professional topics~History of programming languages</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~General programming languages}
\ccsdesc[300]{Social and professional topics~History of programming languages}
%% End of generated code

%% Keywords
%% comma separated list
\keywords{keyword1, keyword2, keyword3}
\fi

%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle

\section{Introduction}

Ever since it was decreed that attention was all you needed, the attention mechanism in large language models has been heavily iterated upon at every level of abstraction. Variations can range from the high-level mathematical domain via activation functions or score modifiers, to the low-level hardware domain introducing tensor operations, quantizations and entirely new floating point representations. Despite being a rather simple mechanism, every step of the mechanism is being optimized by teams of researchers and engineers, and no single abstraction can effectively enable all of these variants at all abstraction levels.

The majority of machine learning research is taught and researched in high-level tensor languages such as PyTorch or TensorFlow
\ivan{todo: find statistics to back this up}. Features such as automatic shape broadcasting and $N$-dimensional vectors keep the mathematics clean and visible, which allow quick iteration by introducing score modifiers, alternative activation functions, quantization, etc. However, the trouble with using such a high level representation is that it is not trivial to automatically optimize for the hardware 
\ivan{which we will cover in a later section}.

On the other hand, when it comes to maximizing the usage of the hardware, close-to-the-metal kernel programming languages such as CUDA and OpenCL tended to be used. These kernel programming languages operated on the single-program, multiple-data (SPMD) model, where each thread is assigned a single program instance. Along with hardware-specific instructions, they allowed programmers to describe workgroups, reasonining about the parallelism of the program and definining exactly which data each thread would fetch, and how the threads would cooperate and synchronize. With such low-level semantics, however, the mathematics of such a kernel was obscured and tended to be difficult to iterate upon.

Block programming languages (eg. Triton, TileIR, etc.) were devised as a middle-ground solution, bringing tensor operations and shape broadcasts to the kernel languages while retaining the pointer arithmetic and workgroup distribution. However, rather than operate on individual threads, programmers now reasoned about individual workgroups, leaving the distribution and synchronization to the compiler to handle. This approach was incredibly successful, and even PyTorch's own attention mechanism is written as a Triton kernel. However, by retaining both mathematical and hardware semantics, block languages also retain the issue of blurring the mathematics of the program and calcifying the program against further experimentation, albeit to a lower degree. As a bottom-up approach, block programming languages enabled kernel programmers to more easily write kernels, however, they did not lower the barrier to high performance code for high-level programmers.

The machine learning compiler community, as well as this paper, adopts a top-down approach. Rather than introducing high-level semantics to a low-level representation, machine learning compilers heavily utilize tiling to automatically apply hardware optimizations such as block-level parallelism and vectorization, while retaining the high-level representation for the frontend programmer. The result is a program where the mathematics is simple and easily iterable, although in practice, a compiler-optimized kernel can never reach the speed of a hand-written kernel, only get close.

Our paper describes a technique for writing attention in a tensor programming language such that the compiler is able to tile it. In particular, we isolate the exponentiation pattern in the softmax mechanism and transform it into an online algorithm, as per the work of Milakov et al.~\cite{milakov_online_2018}. We chose to isolate our problem to attention as this rewrite significantly changes the floating point arithmetic of the program, preventing the rewrite from being applied in generic situations. Even so, attention as a field of research still garners enough effort that it makes sense to target it specifically.

\vspace{.5em}
\noindent
Therefore, our contributions are as follows:
\begin{itemize}
	% \item Contribution 1 (\autoref{sec:implementation})
	\item A technique for writing attention such that compilers are able to easily tile them
  \item A compiler implementation that can apply such block-level optimizations to said tensor program.
\end{itemize}

\section{Background}

% Attention is just softmax(Q * K.T) @ V
%
% We will write this in linalg to demonstrate the higher level abstraction
%    - linalg.matmul
%    - linalg.softmax
%        - reduction to find max
%        - reduction to find sum
%        - elemwise to divide by e^(sum - max)
%    - linalg.matmul
%
% Online softmax replaces the softmax operation
%   - Why can't you automatically fuse the reduction loops of softmax?
%   - Know that sometimes, you won't want to do this, because online softmax also changes the numerics
%
% Flash Attention 2 optimization is the fusion and subsequent tiling of the reduction loops

\subsection{Attention}

% A multitude of attention variants  have sprung up in lieu of the observation that attention is the primary mechanism in Large Language Models\cite{vaswani_attention_2023}. In particular, we isolate the Flash Attention algorithm as the target of our optimization, as it expresses attention in the form of perfect loop nests with affine indexing. This form is easy for compilers to apply classical optimization techniques such as tiling and loop fusion, and has been encoded in the MLIR framework as the `linalg' dialect.

\begin{equation}
  \label{eqn:attention}
  \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}(\frac{\mathbf{Q} \mathbf{K}^T}{\sqrt{d_k}}) \mathbf{V}
\end{equation}
\begin{equation}
  \label{eqn:dot-attention}
  \mathbf{S} = \mathbf{Q} \mathbf{K}^T, \quad \mathbf{P} = \text{softmax}(\mathbf{S}), \quad \mathbf{O} = \mathbf{P} \mathbf{V}
\end{equation}

Attention has been observed to be the primary mechanism in large language models~\cite{vaswani_attention_2023}. While scaled dot product attention (as expressed in \autoref{eqn:attention}) is the most common, a litany of attention variants (\cite{dao_flashattention_2022, dao_flashattention-2_2023}) have arisen ever since. The focus of this paper will be on the Flash Attention~2 \cite{dao_flashattention-2_2023} variant, which uses dot product attention (\autoref{eqn:dot-attention}), and will demonstrate how other attention variants are derivable from it.

\subsection{Tensor and Block Programming}

The naive approach to writing an attention variant is to use a tensor programming language. Typically, a tensor language supports the following features:
\begin{itemize}
  \item Element-wise tensor operations
  \item Linear algebra operations (matrix multiplication, dot product)
  \item Shape operations (slicing, reshaping, automatic broadcast)
\end{itemize}
This small operation set is supported by the DSLs of many machine learning frameworks, such as PyTorch, TensorFlow and cuDNN.

While suitable for programming neural networks and other scientific programs, the semantics of tensor programming languages are too high level to properly optimize for the hardware, in particular, GPUs. Diametrically opposed are thread programming languages such as CUDA, OpenCL and compute shaders, which operate on a SIMT model. SIMT programming languages allow programmers to properly specify and tile the work for the hardware, at the cost of losing the expressibility of tensor languages. 

\paragraph{Block programming languages} To both capture the high-level semantics of tensor languages and the low-level control afforded by thread languages, block programming languages such as Triton were devised. Where thread languages operate on a SPMD model, designating a program ID per thread, block programming languages are defined per block. Each block is assigned a program ID, and corresponds to a work group in OpenCL or a thread block in CUDA. The thread distribution of a block is left up to the compiler to designate. Block programming languages have since supplanted low-level thread languages' place for writing tensor kernels.

While block programming languages prove to make writing linear algebra kernels easier for performance engineers, the same cannot be said for research engineers writing in tensor languages. The optimizations calcify the mathematics into code, making iteration and experimentation on the mathematics of the program much more tedious. Within the field of exploring attention variants, our work provides a framework to make it easier for compilers to easily optimize the attention mechanism itself, while retaining only the high-level semantics of tensor semantics. \ivan{i like the transition, i don't like the words.}

\section{Implementation}
\label{sec:implementation}

\begin{algorithm}
  \caption{Attention, expressed as a series of for loops\label{alg:dot-attention}}
  \begin{algorithmic}[1]
  \Function{Attention}{
    $Q : \mathbb{R}^{N \times d}$,
    $K : \mathbb{R}^{N \times d}$,
    $V : \mathbb{R}^{N \times d}$}

    \State $S, P : \mathbb{R}^{N \times N} \qquad 
            O : \mathbb{R}^{N \times d} \qquad
            m, \sigma :\mathbb{R}^N$
    \State Initialize $m = (-\infty)^N \quad \sigma = (0)^N$,

    % matmul
    \State \emph{Computing $S$ as per \autoref{eqn:dot-attention}}
    \ForAll {$i,j \in [N,N]$}  
      \ForAll {$k \in [d]$}  
        \State $S_{i,j} := S_{i,j} + Q_{i,k} \cdot K_{j,k}$
      \EndFor
    \EndFor

    % softmax
    \State \emph{Calculate the row maximum}
    \ForAll {$i \in [N]$} 
      \ForAll {$j \in [N]$} 
        \State $m_i := \text{max}(m_i, S_{i,j})$
      \EndFor
    \EndFor

    \State \emph{Calculate the softmax denominator}
    \ForAll {$i \in [N]$} 
      \ForAll {$j \in [N]$} 
        \State $\sigma_i := \sigma_i + e^{m_i - S_{i,j}}$
      \EndFor
    \EndFor

    \State \emph{Scale the rows by the softmax denominator}
    \ForAll {$i \in [N]$} 
      \ForAll {$j \in [N]$} 
        \State $P_{i,j} = (e^{m_i - S_{i,j}}) / \sigma_i$
      \EndFor
    \EndFor

    %matmul
    \State \emph{Computing $O$ as per \autoref{eqn:dot-attention}}
    \ForAll {$i,j \in [N,d]$}  
      \ForAll {$k \in [N]$}  
        \State $O_{i,j} = P_{i,k} \cdot V_{k,j}$
      \EndFor
    \EndFor

    \Return $O$
  \EndFunction
  \end{algorithmic}
\end{algorithm}

In this section, we will describe how describing attention as a series of perfect loop nests makes it far easier for the compiler to automatically optimize the mechanism.

First, rather than work with the mathematical description of attention as per 
\autoref{eqn:dot-attention}, the transformations we are applying will be far clearer being written as an algorithm, presented in \autoref{alg:dot-attention}.


\subsection{Compiler Transformations}
In the MLIR compiler framework, the tensor programming language is expressed using the `linalg` dialect \cite{noauthor_linalg_nodate}. This dialect encodes operations as sets of \emph{perfect loop nests} with affine indexing, with each iterating loop marked as either a \emph{parallel} or \emph{reduction} loop. These semantics were inherited from Tensor Comprehensions 
\cite{noauthor_linalg_nodate, vasilache_tensor_2018}, which are in turn, derived from Einstein summation notation.

A producing parallel operation can be trivially merged into a consuming parallel operation if the iteration domain matches.
However, the same cannot be said of reduction loops. Due to intra-loop dependencies, a producer-consumer pair of reductions is non-trivial to fuse manually, let alone automatically. 

\ivan{i REALLY don't like this part and i want to rewrite it}

\subsection{Target Rewrite}

In the end, we want to write attention such that it achieves the form in \autoref{alg:fused-dot-attention}, which is derived from \autoref{alg:dot-attention} by fusing the reduction loops into a single reduction loop. To do this, we applied the exponentiation pattern observed in the work of Milakov et al.~\cite{milakov_online_2018} on the online softmax algorithm, as observed in lines 
Lines 12 and 13. We also moved the softmax division by the accumulator into a separate loop, to avoid dividing multiple times.

\begin{algorithm}
  \caption{Attention, expressed as two matrix multiplications.\label{alg:fused-dot-attention}}
  \begin{algorithmic}[1]
  \Function{Attention}{
    $Q : \mathbb{R}^{N \times d}$,
    $K : \mathbb{R}^{N \times d}$,
    $V : \mathbb{R}^{N \times d}$}

    \State $S : \mathbb{R}^{N \times N} \qquad 
            O : \mathbb{R}^{N \times d} \qquad
            m, \sigma :\mathbb{R}^N$
    \State Initialize $m = (-\infty)^N \quad \sigma = (0)^N$,

    % matmul
    \State \emph{Computing $S$ as per \autoref{eqn:dot-attention}}
    \ForAll {$i,j \in [N,N]$}  
      \ForAll {$k \in [d]$}  
        \State $S_{i,j} := S_{i,j} + Q_{i,k} \cdot K_{j,k}$
      \EndFor
    \EndFor


    % softmax
    \ForAll {$i \in [N]$}
      \ForAll {$j \in [N]$}
        \State $m_i := \text{max}(m_i^{j-1}, S_{i,j})$
        \State $\sigma_i^j := \sigma_i^{j-1} \times e^{m_i - m_i^{j-1}} + e^{S_{i,j} - m_i}$
        \State $O_{i,j} := O_{i,j} \times e^{m_i - m_i^{j-1}} + e^{S_{i,j} - m_i} \cdot V_{j}$
      \EndFor
    \EndFor

    \ForAll {$i,j \in [N,N]$} 
      \State $O_{i,j} := O_{i,j} / \sigma_i$
    \EndFor

    \Return $O$
  \EndFunction
  \end{algorithmic}
\end{algorithm}

\ivan{i wonder if by doing so, autodiff of attention is also easier to derive/optimize?}

\subsection{Loop tiling, loop fusion}

\section{Related Work}

% \lipsum[1-3]


\grosser{Related work should always be at the end of the document,
         as it otherwise becomes an obstacle your reader must
	 overcome before reaching your idea. For details see:
	 \url{https://www.microsoft.com/en-us/research/academic-program/write-great-research-paper/}
}

\section{Conclusion}


%% Acknowledgments
\begin{acks}                            %% acks environment is optional
                                        %% contents suppressed with 'anonymous'
  %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
  %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
  %% acknowledge financial support and will be used by metadata
  %% extraction tools.
  % This material is based upon work supported by the
  % \grantsponsor{GS100000001}{National Science
  %   Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
  % No.~\grantnum{GS100000001}{nnnnnnn} and Grant
  % No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
  % conclusions or recommendations expressed in this material are those
  % of the author and do not necessarily reflect the views of the
  % National Science Foundation.
\end{acks}

%% Bibliography
\bibliography{references}


\end{document}
