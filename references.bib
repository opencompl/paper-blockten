
@misc{kwon_efficient_2023,
	title = {Efficient {Memory} {Management} for {Large} {Language} {Model} {Serving} with {PagedAttention}},
	url = {http://arxiv.org/abs/2309.06180},
	doi = {10.48550/arXiv.2309.06180},
	abstract = {High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4× with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM’s source code is publicly available at https://github.com/vllm-project/vllm.},
	language = {en},
	urldate = {2025-05-23},
	publisher = {arXiv},
	author = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph E. and Zhang, Hao and Stoica, Ion},
	month = sep,
	year = {2023},
	note = {arXiv:2309.06180 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, notion},
	file = {PDF:/Users/ihkh2/Zotero/storage/R3UEECSF/Kwon et al. - 2023 - Efficient Memory Management for Large Language Model Serving with PagedAttention.pdf:application/pdf},
}

@misc{dao_flashattention-2_2023,
	title = {{FlashAttention}-2: {Faster} {Attention} with {Better} {Parallelism} and {Work} {Partitioning}},
	shorttitle = {{FlashAttention}-2},
	url = {http://arxiv.org/abs/2307.08691},
	doi = {10.48550/arXiv.2307.08691},
	abstract = {Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4\${\textbackslash}times\$ compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40{\textbackslash}\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2\${\textbackslash}times\$ speedup compared to FlashAttention, reaching 50-73{\textbackslash}\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72{\textbackslash}\% model FLOPs utilization).},
	urldate = {2025-05-23},
	publisher = {arXiv},
	author = {Dao, Tri},
	month = jul,
	year = {2023},
	note = {arXiv:2307.08691 [cs]},
	keywords = {Computer Science - Machine Learning, notion},
	file = {Preprint PDF:/Users/ihkh2/Zotero/storage/JFWBDD5F/Dao - 2023 - FlashAttention-2 Faster Attention with Better Parallelism and Work Partitioning.pdf:application/pdf;Snapshot:/Users/ihkh2/Zotero/storage/85Z5A859/2307.html:text/html},
}

@misc{liu_ring_2023,
	title = {Ring {Attention} with {Blockwise} {Transformers} for {Near}-{Infinite} {Context}},
	url = {http://arxiv.org/abs/2310.01889},
	doi = {10.48550/arXiv.2310.01889},
	abstract = {Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.},
	urldate = {2025-05-23},
	publisher = {arXiv},
	author = {Liu, Hao and Zaharia, Matei and Abbeel, Pieter},
	month = nov,
	year = {2023},
	note = {arXiv:2310.01889 [cs]},
	keywords = {notion, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/ihkh2/Zotero/storage/IQNEYM5D/Liu et al. - 2023 - Ring Attention with Blockwise Transformers for Near-Infinite Context.pdf:application/pdf;Snapshot:/Users/ihkh2/Zotero/storage/EWKQXWE4/2310.html:text/html},
}

@misc{ye_flashinfer_2025,
	title = {{FlashInfer}: {Efficient} and {Customizable} {Attention} {Engine} for {LLM} {Inference} {Serving}},
	shorttitle = {{FlashInfer}},
	url = {http://arxiv.org/abs/2501.01005},
	doi = {10.48550/arXiv.2501.01005},
	abstract = {Transformers, driven by attention mechanisms, form the foundation of large language models (LLMs). As these models scale up, efficient GPU attention kernels become essential for high-throughput and low-latency inference. Diverse LLM applications demand flexible and high-performance attention solutions. We present FlashInfer: a customizable and efficient attention engine for LLM serving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse format and composable formats to optimize memory access and reduce redundancy. It also offers a customizable attention template, enabling adaptation to various settings through Just-In-Time (JIT) compilation. Additionally, FlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user requests while maintaining compatibility with CUDAGraph which requires static configuration. FlashInfer have been integrated into leading LLM serving frameworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and end-to-end evaluations demonstrate FlashInfer's ability to significantly boost kernel performance across diverse inference scenarios: compared to state-of-the-art LLM serving solutions, FlashInfer achieve 29-69\% inter-token-latency reduction compared to compiler backends for LLM serving benchmark, 28-30\% latency reduction for long-context inference, and 13-17\% speedup for LLM serving with parallel generation.},
	urldate = {2025-04-23},
	publisher = {arXiv},
	author = {Ye, Zihao and Chen, Lequn and Lai, Ruihang and Lin, Wuwei and Zhang, Yineng and Wang, Stephanie and Chen, Tianqi and Kasikci, Baris and Grover, Vinod and Krishnamurthy, Arvind and Ceze, Luis},
	month = apr,
	year = {2025},
	note = {arXiv:2501.01005 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, notion, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/ihkh2/Zotero/storage/F43YJLC9/Ye et al. - 2025 - FlashInfer Efficient and Customizable Attention Engine for LLM Inference Serving.pdf:application/pdf;Snapshot:/Users/ihkh2/Zotero/storage/88A4Y7DS/2501.html:text/html},
}

@misc{milakov_online_2018,
	title = {Online normalizer calculation for softmax},
	url = {http://arxiv.org/abs/1805.02867},
	doi = {10.48550/arXiv.1805.02867},
	abstract = {The Softmax function is ubiquitous in machine learning, multiple previous works suggested faster alternatives for it. In this paper we propose a way to compute classical Softmax with fewer memory accesses and hypothesize that this reduction in memory accesses should improve Softmax performance on actual hardware. The benchmarks confirm this hypothesis: Softmax accelerates by up to 1.3x and Softmax+TopK combined and fused by up to 5x.},
	urldate = {2025-04-23},
	publisher = {arXiv},
	author = {Milakov, Maxim and Gimelshein, Natalia},
	month = jul,
	year = {2018},
	note = {arXiv:1805.02867 [cs]},
	keywords = {notion, Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Performance},
	file = {Preprint PDF:/Users/ihkh2/Zotero/storage/87V5EJKL/Milakov and Gimelshein - 2018 - Online normalizer calculation for softmax.pdf:application/pdf;Snapshot:/Users/ihkh2/Zotero/storage/9X93282H/1805.html:text/html},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	language = {en},
	urldate = {2025-04-22},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Machine Learning, notion, Computer Science - Computation and Language},
	file = {PDF:/Users/ihkh2/Zotero/storage/FR2VCBAZ/Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf},
}

@misc{dao_flashattention_2022,
	title = {{FlashAttention}: {Fast} and {Memory}-{Efficient} {Exact} {Attention} with {IO}-{Awareness}},
	shorttitle = {{FlashAttention}},
	url = {http://arxiv.org/abs/2205.14135},
	doi = {10.48550/arXiv.2205.14135},
	abstract = {Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading oﬀ model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IOaware—accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15\% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3 speedup on GPT-2 (seq. length 1K), and 2.4 speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classiﬁcation) and entirely new capabilities: the ﬁrst Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4\% accuracy) and Path-256 (seq. length 64K, 63.1\% accuracy).},
	language = {en},
	urldate = {2025-04-22},
	publisher = {arXiv},
	author = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and Ré, Christopher},
	month = jun,
	year = {2022},
	note = {arXiv:2205.14135 [cs]},
	keywords = {Computer Science - Machine Learning, notion},
	file = {PDF:/Users/ihkh2/Zotero/storage/BMXHLFSV/Dao et al. - 2022 - FlashAttention Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf:application/pdf},
}

@misc{lucke_mlir_2024,
	title = {The {MLIR} {Transform} {Dialect}. {Your} compiler is more powerful than you think},
	url = {http://arxiv.org/abs/2409.03864},
	doi = {10.48550/arXiv.2409.03864},
	abstract = {To take full advantage of a specific hardware target, performance engineers need to gain control on compilers in order to leverage their domain knowledge about the program and hardware. Yet, modern compilers are poorly controlled, usually by configuring a sequence of coarse-grained monolithic black-box passes, or by means of predefined compiler annotations/pragmas. These can be effective, but often do not let users precisely optimize their varying compute loads. As a consequence, performance engineers have to resort to implementing custom passes for a specific optimization heuristic, requiring compiler engineering expert knowledge. In this paper, we present a technique that provides fine-grained control of general-purpose compilers by introducing the Transform dialect, a controllable IR-based transformation system implemented in MLIR. The Transform dialect empowers performance engineers to optimize their various compute loads by composing and reusing existing - but currently hidden - compiler features without the need to implement new passes or even rebuilding the compiler. We demonstrate in five case studies that the Transform dialect enables precise, safe composition of compiler transformations and allows for straightforward integration with state-of-the-art search methods.},
	language = {en},
	urldate = {2025-07-16},
	publisher = {arXiv},
	author = {Lücke, Martin Paul and Zinenko, Oleksandr and Moses, William S. and Steuwer, Michel and Cohen, Albert},
	month = sep,
	year = {2024},
	note = {arXiv:2409.03864 [cs]},
	keywords = {Computer Science - Programming Languages},
	file = {PDF:/Users/ihkh2/Zotero/storage/BERR677Q/Lücke et al. - 2024 - The MLIR Transform Dialect. Your compiler is more powerful than you think.pdf:application/pdf},
}

@misc{noauthor_mlir_2022,
	title = {{MLIR} {Linalg} {Dialect} and {Patterns}},
	url = {https://www.lei.chat/posts/mlir-linalg-dialect-and-patterns/},
	abstract = {I explained the Vector dialect and related patterns in the previous blog
post. In this one let us look at a layer higher and
talk about the Linalg dialect and transformations around it.},
	language = {en},
	urldate = {2025-07-16},
	journal = {Lei.Chat()},
	month = aug,
	year = {2022},
	note = {Section: posts},
	file = {Snapshot:/Users/ihkh2/Zotero/storage/X3BEW9XC/mlir-linalg-dialect-and-patterns.html:text/html},
}
